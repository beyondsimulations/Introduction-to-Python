---
title: "Lecture VII - Pandas and AI"
subtitle: "Programming with Python"
author: "Dr. Tobias Vlćek"
institute: "Kühne Logistics University Hamburg - Fall 2025"
format:
  revealjs:
    footer: " {{< meta title >}} | {{< meta author >}} | [Home](lec-08-pandas.qmd)"
    output-file: lec-08-presentation.html
---

# [Quick Recap of the last Lecture]{.flow} {.title}

## What is NumPy?
- NumPy is a package for scientific computing in Python
- Provides [multi-dimensional arrays and matrices]{.highlight}
- Much faster than Python lists for numerical operations
- Operations are implemented in C and C++

. . .

:::{.callout-tip}
NumPy arrays are stored in contiguous memory blocks, making operations very efficient.
:::

## Creating Arrays
- Core data structure is the `ndarray`
- Can create arrays from lists, tuples, or other data structures
- Special functions like:
    - `np.zeros()` for arrays of zeros
    - `np.random.rand()` for random values
    - `np.arange()` for evenly spaced values
    - `np.linspace()` for linearly spaced values

## Working with Arrays
- Support for multi-dimensional operations
- Common operations:
    - Element-wise arithmetic (`+`, `-`, `*`, `/`)
    - Array indexing and slicing
    - Shape manipulation (`reshape`, `flatten`)
    - Sorting and transposing

. . .

:::{.callout-tip}
NumPy operations are vectorized, meaning they operate on entire arrays at once rather than element by element.
:::

## NumPy in Action I

[Task]{.task}: Complete the following task:

```{python}
#| eval: false
# TODO: Create an array with 10 evenly spaced numbers over the interval from 0 to 73.

import numpy as np
# YOUR CODE HERE

```

. . .

:::{.callout-note}
Note, that you can always use the `help()` function to get more information about a function. But be sure to import the package first, otherwise you will get an error. To quit the help page, press `q`.
:::

## NumPy in Action II

[Task]{.task}: Complete the following task:

```{python}
#| eval: false
# TODO: Take the following 3x3 array and reduce it to a 1D array.

import numpy as np
array = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
# YOUR CODE HERE

```

# [Pandas Basics]{.flow} {.title}

## What is Pandas?

- Pandas is a [data manipulation and analysis library]{.highlight}
- It provides data structures like **DataFrames and Series**
- Tools for data cleaning, analysis, and visualization
- It can also be used to [work with Excel files!]{.highlight}

## How to install Pandas

- In the last lecture, we have installed it with `pip install pandas` or with Thonny
- Now, import the package `import pandas as pd`

. . .

:::{.callout-note}
You can also use a different abbreviation, but `pd` is the most common one.
:::

## Creating DataFrames

- DataFrames behave quite similar to Numpy arrays
- But they have [row and column labels]{.highlight}

. . .

```{python}
#| eval: true
#| output-location: fragment
import pandas as pd
df = pd.DataFrame({ # DataFrame is created from a dictionary
    "Name": ["Tobias", "Robin", "Nils", "Nikolai"],
    "Kids": [2, 1, 0, 0],
    "City": ["Oststeinbek", "Oststeinbek", "Hamburg", "Lübeck"],
    "Salary": [3000, 3200, 4000, 2500]}); print(df)
```

## Reading from CSV Files

```{python}
#| eval: true
#| output-location: fragment
df = pd.read_csv("supplementary/lec_08/employees.csv") # Reads the CSV file
print(df)
```

## Basic Operations

- Use the `df.head()` method to display the first 5 rows
- Use the `df.tail()` method to display the last 5 rows

. . .

```{python}
#| eval: true
#| output-location: fragment
df = pd.read_csv("supplementary/lec_08/employees.csv")
print(df.tail())
```


## Information about the DataFrame

- Use `df.info()` to display information about a DataFrame

. . .

```{python}
#| eval: true
#| output-location: fragment
df = pd.read_csv("supplementary/lec_08/employees.csv")
print(df.info())
```

## Statistics about a DataFrame

- Use `df.describe()` to display summary statistics
- Use the `df.index` **attribute** to access the **index**

. . .

```{python}
#| eval: true
#| output-location: fragment
df = pd.read_csv("supplementary/lec_08/employees.csv")
print(df.describe())
```

## Filtering DataFrames

- Use `df['column_name']` to access a column
- Use the `df[df['column'] > value]` method to filter

. . .

```{python}
#| eval: true
#| output-location: fragment
df = pd.read_csv("supplementary/lec_08/employees.csv")
df_high_salary = df[df['Salary'] >= 67000]
print(df_high_salary)
print(df_high_salary.iloc[2]["Name"]) #Access the third row and the "Name" column
print(df_high_salary.loc[40]["Name"]) #Access the label 40 and the "Name" column
```

## Filtering in Action

[Task]{.task}: Complete the following task:

```{python}
#| eval: false

# TODO: Load the employees.csv located in the git repository into a DataFrame
# First, filter the DataFrame for employees with a manager position
# Then, print the average salary of the remaining employees
# Finally, print the name of the employee with the lowest salary
```

. . .

:::{.callout-note}
Note, that we can use the `mean()` method on the `Salary` column, as it is a numeric column. In addition, we can use the `min()` method on the `Salary` column to find the lowest salary.
:::

# [Grouping DataFrames]{.flow} {.title}

## Grouping

- Grouping is a [powerful feature]{.highlight} of Pandas
- Groups data by one or more columns
- And then [perform operations]{.highlight}
- Syntax is `df.groupby('column').method()`

. . .

```{python}
#| eval: true
#| output-location: slide
df = pd.read_csv("supplementary/lec_08/employees.csv")
df.groupby(['Position']).sum() # Sum per position
```

## Grouping Numeric Columns

- To prevent errors, we can [select numeric columns first]{.highlight}
- Afterwards, perform the operation on the **selected columns**
- Helps to avoid errors when grouping by non-numeric columns
- Or **drop columns** by `df.drop(columns=["column"])`

. . .

```{python}
#| eval: true
#| output-location: slide
df = pd.read_csv("supplementary/lec_08/employees.csv")
numeric_cols = df.select_dtypes(include=['number']).columns
print(df.groupby("Position")[numeric_cols].sum())
```

## Grouping by Multiple Columns

- Group by multiple columns `['column1', 'column2']`
- You can use [lists or tuples]{.highlight} to specify multiple columns

. . .

```{python}
#| eval: true
#| output-location: slide
df = pd.read_csv("supplementary/lec_08/employees.csv")
df = df.drop(columns=["Name"])
# Max per position and department
df.groupby(['Position', "Department"]).max()
```

## Grouping with Aggregations

- We can use different aggregation functions:
    - `sum()`: sum of the values
    - `mean()`: mean of the values
    - `max()`: maximum of the values
    - `min()`: minimum of the values
    - `count()`: count of the values

## Pandas in Action

[Task]{.task}: Complete the following task:
```{python}
#| eval: false
# TODO: Load the employees.csv again into a DataFrame
# First, group by the "Position" column and count the employees per position
# Then, group by the "Department" column and calculate the mean of all other columns per department
df = pd.read_csv("supplementary/lec_08/employees.csv")
# Your code here
```

# [Combining DataFrames]{.flow} {.title}

## Concatenating DataFrames

- `pd.concat()` to concatenate along shared columns
```{python}
#| eval: true
#| output-location: fragment
df1 = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]})
df2 = pd.DataFrame({"A": [7, 8, 9], "B": [10, 11, 12]})
df = pd.concat([df1, df2])
print(df)
```

## Joining DataFrames

- Use `pd.join()` to join DataFrames along columns
- Joining is [done on the index]{.highlight} by default!

```{python}
#| eval: true
#| output-location: fragment
df1 = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]}, index=['x', 'y', 'z'])
df2 = pd.DataFrame({"C": [7, 8, 9], "D": [10, 11, 12]}, index=['z', 'y', 'w'])
df = df1.join(df2)
print(df)
```

## Merging DataFrames on Columns

- `pd.merge(df_name, on='column', how='type')`
- merge DataFrames along [shared columns]{.highlight}
- `how` specifies the type of merge
    - `inner`: rows with matching keys in both DataFrames
    - `outer`: rows from both are kept, missing values are filled
    - `left`: rows from the left are kept, missing values are filled
    - `right`: rows from right are kept, missing values are filled

## Outer Merge

```{python}
#| eval: true
#| output-location: fragment
df3 = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]})
df4 = pd.DataFrame({"A": [2, 3, 4], "C": [7, 8, 9]})
df_merged = df3.merge(df4, on="A", how="outer")
print(df_merged)
```

## Merging in Action

[Task]{.task}: Complete the following task:

```{python}
#| eval: false
df1 = pd.DataFrame({
    "Name": ["John", "Alice", "Bob", "Carol"],
    "Department": ["Sales", "IT", "HR", "Sales"],
    "Salary": [50000, 60000, 55000, 52000]})
df2 = pd.DataFrame({
    "Name": ["Alice", "Bob", "Dave", "Eve"],
    "Position": ["Developer", "Manager", "Analyst", "Developer"],
    "Years": [5, 8, 3, 4]})

# TODO: Merge the two DataFrames on the "Name" column
# Try different types of merges (inner, outer, left, right)
# Observe and describe the differences in the results
```


# [Working with Excel Files]{.flow} {.title}

## Reading Excel Files

- Read using the `pd.read_excel(file_path)` function
- Write using the `df.to_excel(file_path)` method

. . .

```{python}
#| eval: true
import pandas as pd
df = pd.read_csv("supplementary/lec_08/employees.csv")
df.to_excel("supplementary/lec_08/employees.xlsx", index=False)
```

. . .

:::{.callout-note}
Note, that you likely need to install the `openpyxl` package to be able to write Excel files, as it handles the file format.
:::

## Advanced Excel file handling

We can also [specify the sheet name]{.highlight} when reading and writing

```{python}
#| eval: true

# Writes to the Employees sheet and does not include row indices
df.to_excel("supplementary/lec_08/employees.xlsx", sheet_name="Employees", index=False)
```

. . .

```{python}
#| eval: true
#| output-location: fragment
# Reads from the Employees sheet
df = pd.read_excel("supplementary/lec_08/employees.xlsx", sheet_name="Employees")
print(df.head())
```

## Excel in Action

[Task]{.task}: Complete the following task:

```{python}
#| eval: false
# TODO: Load the temperatures.xlsx file into a DataFrame
# Look at the first few rows of the DataFrame
# Then, print the average temperature per city
```

# [Melting DataFrames]{.flow} {.title}

## Melting

- Sometimes, you want to [transform a DataFrame]{.highlight}
- Instead of **wide** format, you want **long** format
- This is useful for certain types of **visualizations**
- And when working with **time series data**

. . .

[Question]{.question}: Anybody ever heard of the terms?

## Wide Format

For example, the following DataFrame is in [wide format]{.highlight}:

```{python}
#| eval: true
#| echo: false
df = pd.read_excel("supplementary/lec_08/temperatures.xlsx")
print(df)
```

## Long Format

The melting process transforms it into the following [long format]{.highlight}:

```{python}
#| eval: true
#| echo: false
df = pd.read_excel("supplementary/lec_08/temperatures.xlsx")
df = pd.melt(df, id_vars=['Date'], var_name='City', value_name='Temperature')
print(df)
```

## How to melt DataFrames

- Use `pd.melt()` to transform from wide to long
- Parameters:
    - `id_vars`: columns to keep
    - `var_name`: name of the new column that will contain the names of the original columns
    - `value_name`: name of the new column that will contain the values of the original columns

. . .

```{python}
#| eval: true
#| output-location: slide
df = pd.read_csv("supplementary/lec_08/employees.csv")
df = pd.melt(df, id_vars=['Position'], var_name='Variables', value_name='Values')
print(df)
```

## Melting in Action

[Task]{.task}: Complete the following task:

```{python}
#| eval: false
# TODO: Load and transform the temperatures.xlsx file by melting it
# Expected output format:
#         Date        City  Temperature
# 0  2024-03-01    Hamburg         7.2
# 1  2024-03-01 Los_Angeles       18.5
# 2  2024-03-01      Tokyo        12.3
# Then, print the maximum temperature per city by grouping by the "City" column
```


# [Programming with AI]{.flow} {.title}

## Using AI to generate code

- Coding by hand is [not the only way to generate code]{.highlight}
- Most likely, a lot of you have already used **ChatGPT**

. . .

<center>
<iframe src="https://giphy.com/embed/0lGd2OXXHe4tFhb7Wh" width="400" height="400" style="" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></p>
</center>



## {background-image="https://unsplash.com/photos/M5tzZtFCOfs/download?ixid=M3wxMjA3fDB8MXxzZWFyY2h8M3x8c2VydmVyc3xlbnwwfHx8fDE3MzA1MDU5NDd8MA&force=true&w=1920"}

::: {.r-fit-text .white .bold}
How do

Large Language

Models work?

:::

::: footer
[Photo by <a href="https://unsplash.com/@tvick">Taylor Vick</a> on Unsplash]{.white}
:::



## Large Language Models (LLMs)

- Think of them like [advanced pattern recognition systems]{.highlight}
- They have "read" **massive amounts of text**
- Books, websites, articles, code, and more
- Text is broken into **tokens**, parts of words or punctuation
- Based on patterns, they can **generate new text**

## Training LLMs

- Imagine learning a language by [reading millions of books]{.highlight}
- Learns patterns in **how words and ideas connect** via tokens
- Interconnected nodes with **weights representing patterns**
- During training, these **weights are adjusted**
- Once trained, **applying** them takes much less ressources

## Pattern Recognition

- [Not like a search engine!]{.highlight}
- When asked, it looks for **relevant patterns** it learned
- Like having a **huge library** in its "memory" to draw from
- It can find **patterns between concepts** and your question
- Knows only limited text at once (**context window**)

## Probability based responses

- After each written token, it predicts ["what should come next?"]{.highlight}
- Like a advanced version of the **word prediction** on your phone
- Chooses the **most likely next token** based on training
- [But can't actually "think" or "understand" like humans]{.highlight}

## Limitations

- **No true understanding** of cause and effect
- Sometimes **makes mistakes or "hallucinates"**
- Mostly only knows what it **was trained on**
- Can **reflect biases** present in training data
- No emotional understanding (but [can simulate responses!]{.highlight})

## Impact on Jobs

::: {.incremental}

- [Question]{.question}: What do you think about their impact on jobs?
- [Question]{.question}: What are the implications for us?
- [Question]{.question}: Can we use them to our advantage?

:::

## (Current) Choices for Programmers

- [Github Copilot](https://github.com/features/copilot): Integrated into VS Code by Microsoft
- [Cursor](https://www.cursor.com/): Fork of VS Code with AI assistance built in
- [Aider](https://aider.chat): Chat interface for AI to write code in the terminal

. . .

::: {.callout-tip}

Currently, [Cursor](https://www.cursor.com/) is my favorite one. But this might change in the future, as there is a lot of competition in this space.

:::

## Installing Cursor

- Go to [Cursor](https://www.cursor.com/)
- **Download** and **install** Cursor
- You will need to create an account
- Some free usage per month, after that you need to pay
- For us, the **free plan should be more than enough**

## Using Cursor

- Open the folder with your tutorial files
- Create a new `.py` file
- Press `Ctrl + L` to open the chat

## Asking for help

[Task]{.task}: Paste the following prompt in to the chat:


*Can you please write me a small number guessing game in python? It should work for one player in the terminal. The player should guess a number between 1-10 and get hints about whether his guess was too large or too small. After 3 tries, end the game if he didn't succeed with a nice message.*

. . .

[Copy the generated code and paste it into your file.]{.highlight}

## More on Cursor

- While working with Cursor, it will **suggest** you code changes
- You can **accept** or **reject** them
- The rest you will **learn by doing!**

. . .

:::{.callout-note}
**And that's it for todays lecture!**\
You now have the basic knowledge to start working with [tabular data and AI!]{.highlight}.
:::

# [Literature]{.flow} {.title}

## Interesting Books

- Downey, A. B. (2024). Think Python: How to think like a computer scientist (Third edition). O’Reilly. [Link to free online version](https://greenteapress.com/wp/think-python-3rd-edition/)
- Elter, S. (2021). Schrödinger programmiert Python: Das etwas andere Fachbuch (1. Auflage). Rheinwerk Verlag.

. . .

For more interesting literature to learn more about Python, take a look at the [literature list](../general/literature.qmd) of this course.
